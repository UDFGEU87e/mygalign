{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import json\n",
    "import os\n",
    "import argparse\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from networkx.readwrite import json_graph\n",
    "from input.data_preprocess import DataPreprocess\n",
    "\n",
    "import utils.graph_utils as graph_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "    \"\"\"\n",
    "    this class receives input from graphsage format with predefined folder structure, the data folder must contains these files:\n",
    "    G.json, id2idx.json, features.npy (optional)\n",
    "\n",
    "    Arguments:\n",
    "    - data_dir: Data directory which contains files mentioned above.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self._load_G()\n",
    "        self._load_id2idx()\n",
    "        self._load_features()\n",
    "        # self.load_edge_features()\n",
    "        print(\"Dataset info:\")\n",
    "        print(\"- Nodes: \", len(self.G.nodes()))\n",
    "        print(\"- Edges: \", len(self.G.edges()))\n",
    "\n",
    "    def _load_G(self):\n",
    "        G_data = json.load(open(os.path.join(self.data_dir, \"G.json\")))\n",
    "        self.G = json_graph.node_link_graph(G_data)\n",
    "        if type(self.G.nodes()[0]) is int:\n",
    "            mapping = {k: str(k) for k in self.G.nodes()}\n",
    "            self.G = nx.relabel_nodes(self.G, mapping)\n",
    "\n",
    "    def _load_id2idx(self):\n",
    "        print(self.data_dir)\n",
    "        id2idx_file = os.path.join(self.data_dir, 'id2idx.json')\n",
    "        conversion = type(self.G.nodes()[0])\n",
    "        self.id2idx = {}\n",
    "        id2idx = json.load(open(id2idx_file))\n",
    "        for k, v in id2idx.items():\n",
    "            print(k)\n",
    "            print(v)\n",
    "            self.id2idx[conversion(k)] = v\n",
    "\n",
    "    def _load_features(self):\n",
    "        self.features = None\n",
    "        feats_path = os.path.join(self.data_dir, 'feats.npy')\n",
    "        if os.path.isfile(feats_path):\n",
    "            self.features = np.load(feats_path)\n",
    "        else:\n",
    "            self.features = None\n",
    "        return self.features\n",
    "\n",
    "    def load_edge_features(self):\n",
    "        self.edge_features= None\n",
    "        feats_path = os.path.join(self.data_dir, 'edge_feats.mat')\n",
    "        if os.path.isfile(feats_path):\n",
    "            edge_feats = loadmat(feats_path)['edge_feats']\n",
    "            self.edge_features = np.zeros((len(edge_feats[0]),\n",
    "                                           len(self.G.nodes()),\n",
    "                                           len(self.G.nodes())))\n",
    "            for idx, matrix in enumerate(edge_feats[0]):\n",
    "                self.edge_features[idx] = matrix.toarray()\n",
    "        else:\n",
    "            self.edge_features = None\n",
    "        return self.edge_features\n",
    "\n",
    "    def get_adjacency_matrix(self, sparse=False):\n",
    "        return graph_utils.construct_adjacency(self.G, self.id2idx, sparse=False)\n",
    "\n",
    "    def get_nodes_degrees(self):\n",
    "        return graph_utils.build_degrees(self.G, self.id2idx)\n",
    "\n",
    "    def get_nodes_clustering(self):\n",
    "        return graph_utils.build_clustering(self.G, self.id2idx)\n",
    "\n",
    "    def get_edges(self):\n",
    "        return graph_utils.get_edges(self.G, self.id2idx)\n",
    "\n",
    "    def check_id2idx(self):\n",
    "        # print(\"Checking format of dataset\")\n",
    "        for i, node in enumerate(self.G.nodes()):\n",
    "            if (self.id2idx[node] != i):\n",
    "                print(\"Failed at node %s\" % str(node))\n",
    "                return False\n",
    "        # print(\"Pass\")\n",
    "        return True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SynDataset:\n",
    "    def __init__(self, num_nodes, p_create_edge, num_feats=0, seed=1, from_graph=None, num_del=0):\n",
    "        if from_graph is None:\n",
    "            self.G = nx.generators.random_graphs.gnp_random_graph(num_nodes, p_create_edge, seed=seed)\n",
    "            self.id2idx = {id: i for i, id in enumerate(self.G.nodes())}\n",
    "            if num_feats > 0:\n",
    "                self.features = np.zeros((len(self.G.nodes()), num_feats))\n",
    "                for i in range(self.features.shape[0]):\n",
    "                    self.features[i][np.random.randint(0, num_feats)] = 1\n",
    "            else:\n",
    "                self.features = None\n",
    "        else:\n",
    "            self.G = from_graph.G.copy()\n",
    "            if num_del > 0:\n",
    "                count_del = 0\n",
    "                self.considernodes = []\n",
    "                self.considernodes2 = []\n",
    "                for node in self.G.nodes():\n",
    "                    if len(self.G.neighbors(node)) > 4:\n",
    "                        for node2 in self.G.neighbors(node):\n",
    "                            if len(self.G.neighbors(node2)) > 2:\n",
    "                                self.G.remove_edge(node, node2)\n",
    "                                self.considernode = node\n",
    "                                self.considernode2 = node2\n",
    "                                self.considernodes.append(node)\n",
    "                                self.considernodes2.append(node2)\n",
    "                                count_del += 1\n",
    "                                if count_del == num_del:\n",
    "                                    break\n",
    "                            if count_del == num_del:\n",
    "                                break\n",
    "                    if count_del == num_del:\n",
    "                        break\n",
    "            array = np.arange(len(self.G.nodes()))\n",
    "            np.random.shuffle(array)\n",
    "            \n",
    "            self.id2idx = {id: array[i] for i, id in enumerate(self.G.nodes())}\n",
    "            if num_feats > 0:\n",
    "                self.features = from_graph.features[array]\n",
    "            self.groundtruth = {array[i]:i for i in range(len(from_graph.G.nodes()))}\n",
    "            self.groundtruth_matrix = np.zeros((len(self.G.nodes()), len(self.G.nodes())))\n",
    "            self.groundtruth_matrix[array, np.arange(len(self.G.nodes()))] = 1\n",
    "\n",
    "        self.edge_features = None\n",
    "        print(\"Dataset info:\")\n",
    "        print(\"- Nodes: \", len(self.G.nodes()))\n",
    "        print(\"- Edges: \", len(self.G.edges()))\n",
    "        \n",
    "\n",
    "    def get_adjacency_matrix(self):\n",
    "        return graph_utils.construct_adjacency(self.G, self.id2idx)\n",
    "\n",
    "    def get_nodes_degrees(self):\n",
    "        return graph_utils.build_degrees(self.G, self.id2idx)\n",
    "\n",
    "    def get_nodes_clustering(self):\n",
    "        return graph_utils.build_clustering(self.G, self.id2idx)\n",
    "\n",
    "    def get_edges(self):\n",
    "        return graph_utils.get_edges(self.G, self.id2idx)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Test loading dataset\")\n",
    "    parser.add_argument('--source_dataset', default=\"/data/allmv_tmdb//allmv/graphsage/\")\n",
    "    parser.add_argument('--target_dataset', default=\"/data/allmv_tmdb/tmdb/graphsage/\")\n",
    "    parser.add_argument('--groundtruth', default=\"/data/allmv_tmdb/dictionaries/groundtruth\")\n",
    "    parser.add_argument('--output_dir', default=\"/data/allmv_tmdb/statistics/\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "def main(args):    \n",
    "    source_dataset = Dataset(args.source_dataset)\n",
    "    target_dataset = Dataset(args.target_dataset)\n",
    "    groundtruth = graph_utils.load_gt(args.groundtruth, source_dataset.id2idx, target_dataset.id2idx, \"dict\")\n",
    "    DataPreprocess.evaluateDataset(source_dataset, target_dataset, groundtruth, args.output_dir)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--source_dataset SOURCE_DATASET]\n",
      "                             [--target_dataset TARGET_DATASET]\n",
      "                             [--groundtruth GROUNDTRUTH]\n",
      "                             [--output_dir OUTPUT_DIR]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/littlemonster/.local/share/jupyter/runtime/kernel-432f89fd-b54e-491f-89db-1df62164ab19.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "    main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
